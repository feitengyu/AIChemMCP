# llm_client.py
# ä½¿ç”¨chatGPT-4oæ¨¡å‹çš„OpenAIå®¢æˆ·ç«¯ï¼Œé›†æˆå·¥å…·åç§°ç¿»è¯‘åŠŸèƒ½
import os
import json
import re
import time
from openai import OpenAI


API_KEY_FILE = "/opt/data/private/src/AIChemMCP/static/OPENAI_API_KEY"
with open(API_KEY_FILE, 'r') as f:
    api_key = f.read().strip()
os.environ["OPENAI_API_KEY"] = api_key


class ToolNameTranslator:
    """å·¥å…·åç§°ç¿»è¯‘å™¨ï¼Œä½¿ç”¨å¤§æ¨¡å‹APIæ‰¹é‡ç¿»è¯‘ä¸­æ–‡å·¥å…·åç§°"""
    
    def __init__(self, model="gpt-4o"):
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")
        
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.translation_cache = {}
        self.load_cache()

    def load_cache(self, cache_file="tool_name_translations.json"):
        """åŠ è½½ç¿»è¯‘ç¼“å­˜"""
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.translation_cache = json.load(f)
                print(f"âœ… åŠ è½½äº† {len(self.translation_cache)} ä¸ªç¼“å­˜çš„å·¥å…·åç§°ç¿»è¯‘")
            except Exception as e:
                print(f"âŒ åŠ è½½ç¿»è¯‘ç¼“å­˜å¤±è´¥: {e}")
                self.translation_cache = {}

    def save_cache(self, cache_file="tool_name_translations.json"):
        """ä¿å­˜ç¿»è¯‘ç¼“å­˜"""
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.translation_cache, f, ensure_ascii=False, indent=2)
            print(f"âœ… ä¿å­˜äº† {len(self.translation_cache)} ä¸ªå·¥å…·åç§°ç¿»è¯‘åˆ°ç¼“å­˜")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¿»è¯‘ç¼“å­˜å¤±è´¥: {e}")

    def translate_tool_names_batch(self, tool_names, batch_size=20):
        """æ‰¹é‡ç¿»è¯‘å·¥å…·åç§°"""
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡ç¿»è¯‘ {len(tool_names)} ä¸ªå·¥å…·åç§°...")
        
        # å…ˆæ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰çš„ç¿»è¯‘
        remaining_names = []
        translations = {}
        
        for name in tool_names:
            if name in self.translation_cache:
                translations[name] = self.translation_cache[name]
            else:
                remaining_names.append(name)
        
        if not remaining_names:
            print("âœ… æ‰€æœ‰å·¥å…·åç§°å·²å­˜åœ¨ç¼“å­˜ä¸­")
            return translations
        
        print(f"ğŸ”„ éœ€è¦ç¿»è¯‘ {len(remaining_names)} ä¸ªæ–°å·¥å…·åç§°")
        
        # åˆ†æ‰¹å¤„ç†å‰©ä½™çš„åç§°
        for i in range(0, len(remaining_names), batch_size):
            batch = remaining_names[i:i + batch_size]
            print(f"ğŸ”„ ç¿»è¯‘æ‰¹æ¬¡ {i//batch_size + 1}/{(len(remaining_names)-1)//batch_size + 1}")
            
            batch_translations = self._translate_batch(batch)
            translations.update(batch_translations)
            
            # æ›´æ–°ç¼“å­˜
            self.translation_cache.update(batch_translations)
            self.save_cache()
            
            # é¿å…APIé™åˆ¶
            time.sleep(1)
        
        print(f"âœ… å®Œæˆæ‰€æœ‰å·¥å…·åç§°ç¿»è¯‘ï¼")
        return translations

    def _translate_batch(self, tool_names, max_retries=3):
        """ç¿»è¯‘å•ä¸ªæ‰¹æ¬¡çš„å·¥å…·åç§°ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
        # æ„å»ºæç¤ºè¯
        prompt = self._build_translation_prompt(tool_names)
        
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ–å­¦å®éªŒå®¤è®¾å¤‡åç§°ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä¸­æ–‡è®¾å¤‡åç§°å‡†ç¡®ç¿»è¯‘æˆè‹±æ–‡ï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ä½¿ç”¨æ ‡å‡†çš„å®éªŒå®¤è®¾å¤‡å‘½åè§„èŒƒ
2. ä¿æŒä¸“ä¸šæ€§ï¼Œä½¿ç”¨å‡†ç¡®çš„ç§‘æŠ€æœ¯è¯­
3. åç§°æ ¼å¼ï¼šåªä½¿ç”¨å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦
4. é¿å…ä½¿ç”¨ç©ºæ ¼å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
5. å¯¹äºå¤åˆè®¾å¤‡ï¼Œä½¿ç”¨æœ‰æ„ä¹‰çš„ç»„åˆåç§°
6. ç¡®ä¿åç§°ç®€æ´ä¸”å…·æœ‰æè¿°æ€§"""
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                )
                
                result_text = response.choices[0].message.content
                translations = self._parse_translation_result(result_text, tool_names)
                
                # éªŒè¯æ‰€æœ‰åç§°éƒ½è¢«ç¿»è¯‘
                missing = set(tool_names) - set(translations.keys())
                if missing:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•ç¼ºå°‘ç¿»è¯‘: {missing}ï¼Œè¿›è¡Œé‡è¯•...")
                        continue
                    else:
                        raise Exception(f"é‡è¯•{max_retries}æ¬¡åä»ç¼ºå°‘ç¿»è¯‘: {missing}")
                
                return translations
                
            except Exception as e:
                print(f"âŒ ç¬¬{attempt+1}æ¬¡ç¿»è¯‘å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                    time.sleep(2)
                else:
                    raise Exception(f"ç¿»è¯‘å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")

    def _build_translation_prompt(self, tool_names):
        """æ„å»ºç¿»è¯‘æç¤ºè¯"""
        names_list = "\n".join([f"- {name}" for name in tool_names])
        
        return f"""
è¯·å°†ä»¥ä¸‹åŒ–å­¦å®éªŒå®¤è®¾å¤‡åç§°ç¿»è¯‘æˆè‹±æ–‡ã€‚

è¦æ±‚è¿”å›çº¯JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
```json
{{
  "åŸåç§°1": "english_name_1",
  "åŸåç§°2": "english_name_2",
  ...
}}

éœ€è¦ç¿»è¯‘çš„è®¾å¤‡åç§°ï¼š
{names_list}
"""

    def _parse_translation_result(self, result_text, original_names):
        """è§£æç¿»è¯‘ç»“æœ"""
        try:
            # å°è¯•ä»ç»“æœä¸­æå–JSON
            if "```json" in result_text:
                json_str = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                json_str = result_text.split("```")[1].strip()
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
                json_str = result_text.strip()
            
            translations = json.loads(json_str)
            
            return translations
            
        except Exception as e:
            print(f"âŒ è§£æç¿»è¯‘ç»“æœå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {result_text}")
            raise Exception(f"è§£æç¿»è¯‘ç»“æœå¤±è´¥: {e}")


class OpenAI_LLM:
    def __init__(self, model="gpt-4o"):
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")

        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.translator = ToolNameTranslator()
        self.tool_name_mapping = {}  # å·¥å…·åç§°æ˜ å°„ç¼“å­˜
        self.sanitized_to_original_mapping = {}  # æ¸…ç†ååç§°åˆ°åŸå§‹åç§°çš„æ˜ å°„

    def _build_openai_tools_and_mapping(self, mcp_tools: dict):
        """æ„å»ºOpenAIå·¥å…·åˆ—è¡¨å’Œåç§°æ˜ å°„ï¼ˆä½¿ç”¨ç¿»è¯‘åçš„åç§°ï¼‰"""
        # è·å–æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„å·¥å…·åç§°
        tool_names = list(mcp_tools.keys())
        
        # æ‰¹é‡ç¿»è¯‘å·¥å…·åç§°
        if not self.tool_name_mapping:
            self.tool_name_mapping = self.translator.translate_tool_names_batch(tool_names)
        
        openai_tools = []
        sanitized_to_original = {}
        used_names = set()
        
        for tool_name, tool_data in mcp_tools.items():
            # ä½¿ç”¨ç¿»è¯‘åçš„åç§°
            if tool_name not in self.tool_name_mapping:
                raise Exception(f"å·¥å…·åç§° '{tool_name}' ç¿»è¯‘å¤±è´¥ï¼Œæ— æ³•æ„å»ºå·¥å…·åˆ—è¡¨")
                
            base_sanitized_name = self.tool_name_mapping[tool_name]
            
            # ç¡®ä¿åç§°å”¯ä¸€
            sanitized_name = base_sanitized_name
            counter = 1
            while sanitized_name in used_names:
                sanitized_name = f"{base_sanitized_name}_{counter}"
                counter += 1
            used_names.add(sanitized_name)
            
            # ä¿å­˜æ˜ å°„
            sanitized_to_original[sanitized_name] = tool_name
            
            # æ„å»ºOpenAIå·¥å…·æ ¼å¼
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": sanitized_name,
                    "description": tool_data.get("description", ""),
                    "parameters": tool_data.get("parameters", {"type": "object", "properties": {}})
                }
            })
        
        # ä¿å­˜æ˜ å°„ä¾›åç»­ä½¿ç”¨
        self.sanitized_to_original_mapping = sanitized_to_original
        
        return openai_tools, sanitized_to_original

    def _find_original_tool_name(self, sanitized_name: str) -> str:
        """å°†æ¸…ç†åçš„å·¥å…·åç§°æ˜ å°„å›åŸå§‹åç§°"""
        return self.sanitized_to_original_mapping.get(sanitized_name, sanitized_name)

    def debug_tool_names(self, mcp_tools: dict):
        """è°ƒè¯•å·¥å…·åç§°ï¼Œæ˜¾ç¤ºåŸå§‹åç§°å’Œæ¸…ç†åçš„åç§°"""
        print("\n=== å·¥å…·åç§°è°ƒè¯•ä¿¡æ¯ ===")
        for tool_name in mcp_tools.keys():
            sanitized = self.tool_name_mapping.get(tool_name, "æœªç¿»è¯‘")
            print(f"åŸå§‹: '{tool_name}' -> æ¸…ç†: '{sanitized}'")
        print("=======================\n")

    def get_decision(self, system_prompt: str, history: list, mcp_tools: dict) -> dict:
        """
        è°ƒç”¨OpenAI APIè·å–LLMçš„å†³ç­–ï¼ˆè¯´è¯æˆ–è°ƒç”¨å·¥å…·ï¼‰ã€‚
        """
        # 1. æ„å»ºOpenAIå·¥å…·åˆ—è¡¨å’Œæ˜ å°„
        openai_tools, sanitized_to_original = self._build_openai_tools_and_mapping(mcp_tools)

        # 2. æ ¼å¼åŒ–å¯¹è¯å†å²
        messages = [{"role": "system", "content": system_prompt}]
        for turn in history:
            role = turn['role']
            content = turn['content']

            if role == "user":
                messages.append({"role": "user", "content": content})
            elif role == "assistant" and "tool_call" in content:
                # å°†æˆ‘ä»¬çš„tool_callæ ¼å¼è½¬æ¢å›OpenAIçš„æ ¼å¼
                tool_call = content['tool_call']
                # æŸ¥æ‰¾æ¸…ç†åçš„å·¥å…·åç§°
                sanitized_method = None
                for sanitized, original in sanitized_to_original.items():
                    if original == tool_call['method']:
                        sanitized_method = sanitized
                        break
                
                if sanitized_method is None:
                    raise Exception(f"æ‰¾ä¸åˆ°å·¥å…· '{tool_call['method']}' çš„ç¿»è¯‘åç§°")
                
                messages.append({
                    "role": "assistant",
                    "tool_calls": [{
                        "id": f"call_{sanitized_method}_{int(time.time())}",
                        "type": "function",
                        "function": {
                            "name": sanitized_method,
                            "arguments": json.dumps(tool_call['params'])
                        }
                    }]
                })
            elif role == "tool_result":
                # éœ€è¦æ‰¾åˆ°å¯¹åº”çš„å·¥å…·è°ƒç”¨ID
                if messages and 'tool_calls' in messages[-1]:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": messages[-1]['tool_calls'][0]['id'],
                        "name": messages[-1]['tool_calls'][0]['function']['name'],
                        "content": json.dumps(content)
                    })

        # 3. å‘èµ·APIè°ƒç”¨
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=openai_tools,
                tool_choice="auto"
            )

            response_message = response.choices[0].message

            # 4. è§£æAPIçš„å“åº”
            if response_message.tool_calls:
                # LLMå†³å®šè°ƒç”¨ä¸€ä¸ªå·¥å…·
                tool_call = response_message.tool_calls[0].function
                # å°†æ¸…ç†åçš„å·¥å…·åç§°æ˜ å°„å›åŸå§‹åç§°
                original_tool_name = self._find_original_tool_name(tool_call.name)
                
                return {
                    "thought": response_message.content or "I should use a tool to proceed.",
                    "tool_call": {
                        "method": original_tool_name,
                        "params": json.loads(tool_call.arguments)
                    }
                }
            else:
                # LLMå†³å®šç›´æ¥ä¸ç”¨æˆ·å¯¹è¯
                return {
                    "thought": "I will respond directly to the user.",
                    "speak": response_message.content
                }
        except Exception as e:
            print(f"[LLM_CLIENT_ERROR] API call failed: {e}")
            return {"speak": "I'm sorry, I encountered an error while processing your request."}

    def generate_plan(self, system_prompt: str, user_goal: str, mcp_tools: dict):
        """è°ƒç”¨OpenAI APIç›´æ¥ç”Ÿæˆplanã€‚"""
        try:
            # è°ƒè¯•ï¼šæ˜¾ç¤ºå·¥å…·åç§°è½¬æ¢
            self.debug_tool_names(mcp_tools)
            
            # 1. æ„å»ºOpenAIå·¥å…·åˆ—è¡¨å’Œæ˜ å°„
            openai_tools, sanitized_to_original = self._build_openai_tools_and_mapping(mcp_tools)
            
            # ä½¿ç”¨æ˜ å°„ä¸­çš„é”®ä½œä¸ºå¯ç”¨çš„å·¥å…·åç§°
            available_tool_names = list(sanitized_to_original.keys())
            
            # 2. åˆ›å»ºä¸€ä¸ª"å®¹å™¨"å·¥å…·ï¼Œå¼ºåˆ¶LLMè¾“å‡ºä¸€ä¸ªè®¡åˆ’åˆ—è¡¨
            plan_schema = {
                "type": "object",
                "properties": {
                    "plan": {
                        "type": "array",
                        "description": "ä¸€ä¸ªåŒ…å«æ‰€æœ‰è®¡åˆ’æ­¥éª¤çš„æœ‰åºåˆ—è¡¨ã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "method": {
                                    "type": "string",
                                    "description": "è¦è°ƒç”¨çš„å·¥å…·åç§°ã€‚",
                                    "enum": available_tool_names
                                },
                                "params": {
                                    "type": "object", 
                                    "description": "ä¼ é€’ç»™å·¥å…·çš„å‚æ•°ã€‚",
                                    "properties": {},
                                    "additionalProperties": True
                                }
                            },
                            "required": ["method", "params"]
                        }
                    }
                },
                "required": ["plan"]
            }

            planner_tool = {
                "type": "function",
                "function": {
                    "name": "submit_workflow_plan",
                    "description": "æäº¤æœ€ç»ˆç”Ÿæˆçš„ã€åŒ…å«å¤šä¸ªæ­¥éª¤çš„å·¥ä½œæµè®¡åˆ’ã€‚",
                    "parameters": plan_schema
                }
            }

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_goal}
            ]

            # 3. å‘èµ·APIè°ƒç”¨ï¼Œå¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬çš„"å®¹å™¨"å·¥å…·
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=[planner_tool],  # åªä¼ é€’è§„åˆ’å·¥å…·ï¼Œä¸ä¼ é€’å…¶ä»–å·¥å…·
                tool_choice={"type": "function", "function": {"name": "submit_workflow_plan"}}
            )

            # 4. æ£€æŸ¥å“åº”å¹¶æå–è®¡åˆ’
            response_message = response.choices[0].message
            
            if not response_message.tool_calls:
                print(f"[LLM_CLIENT_ERROR] No tool call in response: {response_message}")
                return [{"method": "error", "params": {"message": "No plan generated"}}]
                
            tool_call = response_message.tool_calls[0]
            tool_call_args = tool_call.function.arguments
            
            try:
                plan_data = json.loads(tool_call_args)
            except json.JSONDecodeError as e:
                print(f"[LLM_CLIENT_ERROR] JSON decode error: {e}, raw arguments: {tool_call_args}")
                return [{"method": "error", "params": {"message": f"JSON decode error: {e}"}}]
            
            # 5. å°†æ¸…ç†åçš„å·¥å…·åç§°æ˜ å°„å›åŸå§‹åç§°
            original_plan = []
            for step in plan_data.get("plan", []):
                if "method" not in step:
                    print(f"[LLM_CLIENT_ERROR] Step missing 'method': {step}")
                    continue
                    
                original_method = self._find_original_tool_name(step["method"])
                step_params = step.get("params", {})
                
                original_plan.append({
                    "method": original_method,
                    "params": step_params
                })
            
            return original_plan

        except Exception as e:
            print(f"[LLM_CLIENT_ERROR] Plan generation failed: {e}")
            import traceback
            traceback.print_exc()
            return [{"method": "error", "params": {"message": str(e)}}]


# æ‰¹é‡ç¿»è¯‘å·¥å…·
def batch_translate_all_tools(servers_dir="servers_and_tools"):
    """æ‰¹é‡ç¿»è¯‘æ‰€æœ‰å·¥å…·åç§°"""
    import os
    
    # ä»æœåŠ¡å™¨æ–‡ä»¶åä¸­æå–å·¥å…·åç§°
    tool_names = set()
    
    if not os.path.exists(servers_dir):
        print(f"é”™è¯¯: ç›®å½• '{servers_dir}' ä¸å­˜åœ¨")
        return
    
    # éå†æ‰€æœ‰æœåŠ¡å™¨æ–‡ä»¶
    for filename in os.listdir(servers_dir):
        if filename.endswith("_server_v2.py"):
            # ä»æ–‡ä»¶åæå–è®¾å¤‡åç§°ï¼ˆå»æ‰åç¼€ï¼‰
            device_name = filename.replace("_server_v2.py", "")
            # å°†ä¸‹åˆ’çº¿è¿˜åŸä¸ºåŸå§‹åç§°ï¼ˆå‡è®¾åŸå§‹åç§°ä¸­æ²¡æœ‰ä¸‹åˆ’çº¿ï¼‰
            original_name = device_name.replace('_', '')
            tool_names.add(original_name)
    
    tool_names = list(tool_names)
    print(f"ğŸ“‹ æ‰¾åˆ° {len(tool_names)} ä¸ªå·¥å…·åç§°éœ€è¦ç¿»è¯‘")
    
    # æ‰¹é‡ç¿»è¯‘
    translator = ToolNameTranslator()
    translations = translator.translate_tool_names_batch(tool_names)
    
    # ä¿å­˜å®Œæ•´çš„ç¿»è¯‘ç»“æœ
    output_file = "all_tool_translations.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(translations, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç¿»è¯‘å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")
    print("\nç¿»è¯‘ç»“æœé¢„è§ˆ:")
    for cn, en in list(translations.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
        print(f"  {cn} -> {en}")


if __name__ == "__main__":
    # è¿è¡Œæ‰¹é‡ç¿»è¯‘
    batch_translate_all_tools()